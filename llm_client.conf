# llm_client.conf
#
# Configuration for the LLM Bash Client.
#
# --- Global Settings ---
DEFAULT_LLM_MODE="openai" # Default mode: "openai" or "gemini"
DEFAULT_MAX_TOKEN=1024

# --- OpenAI-compatible API Settings ---
OPENAI_API_KEY="tgp_v1_Ur5O4"
OPENAI_API_URL="https://api.together.xyz/v1/chat/completions"
OPENAI_MODEL="mistralai/Mistral-Small-24B-Instruct-2501"
OPENAI_JSON_PATH=".choices[0].message.content" # jq path to extract response

# --- Google Gemini API Settings ---
GEMINI_API_KEY="AIzaSyCTrg"
GEMINI_API_URL="https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent"
GEMINI_MODEL="gemini-2.5-flash" # Note: Model might be part of URL for Gemini
GEMINI_JSON_PATH=".candidates[0].content.parts[0].text" # jq path to extract response

# Add more provider configurations as needed
# For example:
# COHERE_API_KEY="YOUR_COHERE_API_KEY_HERE"
# COHERE_API_URL="https://api.cohere.ai/v1/generate"
# COHERE_MODEL="command"
# COHERE_JSON_PATH=".generations[0].text"
